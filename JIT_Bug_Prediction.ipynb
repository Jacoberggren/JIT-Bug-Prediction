{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data + Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)          # stops pandas from truncating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"datasets/\"))  \n",
    "openstackDataset = pd.read_csv(\"datasets/openstack_metrics.csv\")\n",
    "qtDataset = pd.read_csv(\"datasets/qt_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick datasets for training and inference testing, this allows for easy switching between them\n",
    "trainingDataset = openstackDataset\n",
    "inferenceDataset = qtDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary attributes early since they hinder analysis-functions like correlation matrices\n",
    "trainingDataset = trainingDataset.drop(['commit_id', 'author_date'], axis=1)   \n",
    "inferenceDataset = inferenceDataset.drop(['commit_id', 'author_date'], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding each feature of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 commit_id :      unique id for the commited code                                #16 ndev :   N of devs that have modied the file in the past\n",
    "#1 author_date :    id for the                                                     #17 age :    time from previous update to the file\n",
    "#2 bugcount :       amount of bugs detected (previous bugs? -> rereviews)          #18 nuc :    N of prior(unique) changes to the file\n",
    "#3 fixcount :       amount of fixes                                                #19 app :    N of reviewers who voted on integration\n",
    "#4 la :             lines added                                                    #20 aexp :   N of changes author has participated in\n",
    "#5 ld :             lines deleted                                                  #21 rexp :   aexp but reviewer instead of author\n",
    "#6 nf :             modified files                                                 #22 oexp :   o?\n",
    "#7 nd :             modified directories                                           #23 arexp :  aexp but weighted by recency of changes\n",
    "#8 ns :             modified subsystems                                            #24 rrexp :  arexp but reviewer instead of author\n",
    "#9 ent :            entropy, spread of modified lines across files                 #25 orexp :  o?\n",
    "#10 revd :          reviewed/revised? revd=False -> rxxx=NaN                       #26 asexp :  N of changes within the subsystem from the author\n",
    "#11 nrev :          N of revisions to the commit                                   #27 rsexp :  asexp but reviewer instead of author\n",
    "#12 rtime :         time between commit and approval                               #28 osexp :  o?\n",
    "#13 tcmt :          total comments on the commit?                                  #29 asawr :  Proportion of previous changes to the subsystem from the author\n",
    "#14 hcmt :          N of non-automated comments during review                      #30 rsawr :  asawr but reviewer instead of author\n",
    "#15 self :          only self-checked? inverse of revd                             #31 osawr :  o?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataset.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceDataset.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising and analysing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training dataset:\", trainingDataset.shape)\n",
    "print(\"Shape of training dataset:\", inferenceDataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.isna().sum()           # sums the amount of NaN values for each attribute\n",
    "#data.describe()             # basic statistics for each attribute\n",
    "#data.nunique()              # amount of unique values for each attribute\n",
    "trainingDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of instances where the commit was reviewed/revised in training dataset:\", trainingDataset.revd.value_counts())\n",
    "print(\"Amount of instances where the commit was reviewed/revised in inference dataset:\", inferenceDataset.revd.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spread of bugs found (does not include instances of NaN)\")\n",
    "sns.countplot(x='bugcount', data=trainingDataset)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of instances of bugcount containing NaN:\", trainingDataset.bugcount.isna().sum())\n",
    "print(\"Amount of times bugs were detected:\", trainingDataset.bugcount.value_counts().sum() - trainingDataset.bugcount.value_counts().get(0.0),\n",
    "                    \"out of\", trainingDataset.bugcount.count() + trainingDataset.bugcount.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = trainingDataset.corr()\n",
    "high_correlations = correlation_matrix[(correlation_matrix > 0.9) | (correlation_matrix < -0.9)]\n",
    "# Replace NaN values with an empty string\n",
    "high_correlations.fillna('', inplace=True)\n",
    "\n",
    "# Drop correlation measures for columns with themselves\n",
    "for col in high_correlations.columns:\n",
    "    high_correlations.loc[col, col] = ''\n",
    "# Print correlation for each combination of features\n",
    "for col in high_correlations.columns:\n",
    "    correlations = high_correlations[col][high_correlations[col] != '']\n",
    "    if not correlations.empty:\n",
    "        print(f\"Correlations with {col}:\")\n",
    "        print(correlations)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataset.corr()['revd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on replacing NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances of NaN in bugcount/fixcount are assumed to imply that 0 bugs/fixes were recorded.\n",
    "\n",
    "Based on correlation analysis of ['revd'], the remaining instances of NaN are all from features related to reviews/revisions of the commit\n",
    " \n",
    "These instances are NaN iff ['revd'] = False, implying no review was made to the commit -> all review related features are therefore NaN\n",
    "\n",
    "Replacing NaN with 0 is not equally suitable for all of these features, but is done for simplicity, example:\n",
    "\n",
    "* ['app'] = 0, implies that no reviewers voted on integration when there was no review to vote on, this seems suitable\n",
    "\n",
    "* ['aexp'] = 0, implies the author has participated in 0 previous changes, which is not necessarily true, not so suitable\n",
    "\n",
    "Given more time, this could be better adapted and customized to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all \"r\" and \"o\" feature-types\n",
    "# they contain highly correlated data, increases dimensional complexity, and rank very low on feature importance when training\n",
    "unnecessaryColumns = ['arexp', 'self', 'revd', 'rexp', 'rrexp', 'rsexp', 'rsawr', 'oexp', 'orexp', 'osexp', 'osawr']\n",
    "trainingDataset = trainingDataset.drop(unnecessaryColumns, axis=1)\n",
    "\n",
    "# For the remaining features containing NaN, NaN are assumed to imply 0, see argument above\n",
    "trainingDataset = trainingDataset.fillna(0.0)              \n",
    "\n",
    "# turn bugcount into binary feature instead of multi-class \n",
    "trainingDataset['bugcount'] = trainingDataset['bugcount'].apply(lambda x: 1 if x != 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated cleaning for inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceDataset = inferenceDataset.drop(unnecessaryColumns, axis=1)\n",
    "inferenceDataset = inferenceDataset.fillna(0.0) \n",
    "\n",
    "inferenceDataset['bugcount'] = inferenceDataset['bugcount'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "# extract target feature ['bugcount'] for inference testing\n",
    "inference_bugcount = inferenceDataset.bugcount     \n",
    "inference_set = inferenceDataset.drop(['bugcount'], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Minority Oversampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bugcount is vastly imbalanced, this is solved by increasing the ratio of instances with found bugs. \n",
    "\n",
    "* Oversampling: SMOTE chooses random samples of minority class, finds similar instances of data using k-nearest neighbor and generates a new instance between the original sample and one random neighbor.\n",
    "\n",
    "* Undersampling: Randomly undersamples data with majority class, in this case: bugcount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bugcount\n",
      "0    24887\n",
      "1     1968\n",
      "Name: count, dtype: int64\n",
      "bugcount\n",
      "0    14932\n",
      "1     7466\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import data and extract target feature before balancing \n",
    "training_data = trainingDataset\n",
    "target_feature = training_data.bugcount\n",
    "training_data = training_data.drop(['bugcount'], axis=1)\n",
    "\n",
    "# show class ratio\n",
    "print(target_feature.value_counts())\n",
    "\n",
    "# prepare pipeline for balancing\n",
    "oversample = SMOTE(sampling_strategy=0.3)                   \n",
    "undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "steps = [('over', oversample), ('under', undersample)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# balance bugcount class\n",
    "training_data, target_feature = pipeline.fit_resample(training_data, target_feature)\n",
    "# show new balanced class ratio\n",
    "print(target_feature.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing features and splitting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into test and train sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(training_data, target_feature, test_size=0.1, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing features to values [-1, 1], can help some models accurately depict features during training\n",
    "# Example: SVM models are sensitive to the scale of features, especially SVMs with linear kernels. \n",
    "# This also greatly reduces training times, especially for SVM models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "x_train_standardized = sc.fit_transform(x_train)\n",
    "x_test_standardized = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier model, validated using k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal mean f1_macro: 0.885\n",
      "Trained model f1-score: 0.856\n"
     ]
    }
   ],
   "source": [
    "# specify classifier model\n",
    "rfc_model = RandomForestClassifier(n_estimators=25)     #25->100, marginally better but large impact on compute\n",
    "\n",
    "# define k-fold strategy and perform crossvalidation\n",
    "crossValidation = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)     # 10 splits is a good bias-variance balance\n",
    "scores = cross_val_score(rfc_model, training_data, target_feature, scoring='f1_macro', cv=crossValidation, n_jobs=-1)   \n",
    "print('CrossVal mean f1_macro: %.3f' % (np.mean(scores)))         # f1_macro scores adds weights to results from each class\n",
    "\n",
    "# train RandomForestClassifier model\n",
    "rfc_model.fit(x_train, y_train)\n",
    "\n",
    "# inference on test set\n",
    "rfc_predictions = rfc_model.predict(x_test)\n",
    "print(\"Trained model f1-score: %.3f\" % (f1_score(y_test, rfc_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM f1 score: 0.5499600319744204\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel='linear')        # kernel alternatives: 'rbf', 'poly' and 'sigmoid'\n",
    "svm_model.fit(x_train_standardized, y_train)\n",
    "svm_predictions = svm_model.predict(x_test_standardized)\n",
    "svm_f1 = f1_score(y_test, svm_predictions)\n",
    "print(\"SVM f1 score:\", svm_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes model, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes f1 score: 0.4939467312348668\n"
     ]
    }
   ],
   "source": [
    "# This type of model also performs much better using standardized features\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(x_train_standardized, y_train)\n",
    "nb_predictions = nb_model.predict(x_test_standardized)\n",
    "nb_f1 = f1_score(y_test, nb_predictions)\n",
    "print(\"Naive Bayes f1 score:\", nb_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performance from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for each model: rfc_predictions, svm_predictions or nb_predictions\n",
    "print(classification_report(y_test, rfc_predictions, target_names=['no bugs', 'contains bugs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(rfc_model, x_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performance on inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model alternatives: rfc_model, svm_model or nb_model\n",
    "# Note: Predictions by svm_model and nb_model might give warnings since inference_set is not standardized. \n",
    "inference_results = rfc_model.predict(inference_set)\n",
    "print(classification_report(inference_bugcount, inference_results, target_names=['no bugs', 'contains bugs'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate baseline: what are the odds of finding a bug in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline prediction for different datasets: training_data -> target_feature | inference_data -> inference_bugcount \n",
    "count_zero = inference_bugcount.value_counts().get(0)\n",
    "count_one = inference_bugcount.value_counts().get(1)\n",
    "totalCount = count_one + count_zero\n",
    "chance_zero = count_zero / totalCount\n",
    "chance_one = count_one / totalCount\n",
    "randomGuessAccuracy = (chance_zero + chance_one) / 2\n",
    "weightedGuessAccuracy = (chance_one * chance_one) + (chance_zero * chance_zero)\n",
    "print(\"Bugcount values, 0: %d, 1: %d\" %(count_zero, count_one))\n",
    "print(\"Probability of majority class, p(0): %.3f\" %(chance_zero))\n",
    "print(\"Propability of minority class, p(1): %.3f\" %(chance_one))\n",
    "print(\"Accuracy of a random guess: %.3f\" % randomGuessAccuracy)\n",
    "print(\"Accuracy of a weighted guess: %.3f\" % weightedGuessAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(rfc_model.feature_importances_, index=x_train.columns).sort_values(axis = 0, ascending = False)\n",
    "feature_importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
