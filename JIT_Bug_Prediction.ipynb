{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data + Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to execute this project is simply run all cells. The datasets are included in the folder contained in the same zip-archive.\n",
    "\n",
    "The datasets used for training and inference can be alternated in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)          # stops pandas from truncating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"datasets/\"))  \n",
    "openstackDataset = pd.read_csv(\"datasets/openstack_metrics.csv\")\n",
    "qtDataset = pd.read_csv(\"datasets/qt_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick datasets for training and inference testing, this allows for easy switching between them\n",
    "trainingDataset = openstackDataset\n",
    "inferenceDataset = qtDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary attributes early since they hinder analysis-functions like correlation matrices\n",
    "trainingDataset = trainingDataset.drop(['commit_id', 'author_date'], axis=1)   \n",
    "inferenceDataset = inferenceDataset.drop(['commit_id', 'author_date'], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding each feature of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 commit_id :      unique id for the commited code                                #16 ndev :   N of devs that have modied the file in the past\n",
    "#1 author_date :    id for the author of the commit                                #17 age :    time since previous update to the file\n",
    "#2 bugcount :       amount of bugs detected                                        #18 nuc :    N of prior(unique) changes to the file\n",
    "#3 fixcount :       amount of fixes                                                #19 app :    N of reviewers who voted on integration\n",
    "#4 la :             lines added                                                    #20 aexp :   N of changes author has participated in\n",
    "#5 ld :             lines deleted                                                  #21 rexp :   aexp but reviewer instead of author\n",
    "#6 nf :             modified files                                                 #22 oexp :   o?\n",
    "#7 nd :             modified directories                                           #23 arexp :  aexp but weighted by recency of changes\n",
    "#8 ns :             modified subsystems                                            #24 rrexp :  arexp but reviewer instead of author\n",
    "#9 ent :            entropy, spread of modified lines across files                 #25 orexp :  o?\n",
    "#10 revd :          reviewed/revised? revd=False -> rxxx=NaN                       #26 asexp :  N of changes within the subsystem from the author\n",
    "#11 nrev :          N of revisions to the commit                                   #27 rsexp :  asexp but reviewer instead of author\n",
    "#12 rtime :         time between commit and approval                               #28 osexp :  o?\n",
    "#13 tcmt :          total comments on the commit?                                  #29 asawr :  Proportion of previous changes to the subsystem from the author\n",
    "#14 hcmt :          N of non-automated comments during review                      #30 rsawr :  asawr but reviewer instead of author\n",
    "#15 self :          only self-checked? inverse of revd                             #31 osawr :  o?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataset.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceDataset.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising and analysing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training dataset:\", trainingDataset.shape)\n",
    "print(\"Shape of training dataset:\", inferenceDataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.isna().sum()           # sums the amount of NaN values for each attribute\n",
    "#data.describe()             # basic statistics for each attribute\n",
    "#data.nunique()              # amount of unique values for each attribute\n",
    "trainingDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of instances where the commit was reviewed/revised in training dataset:\", trainingDataset.revd.value_counts())\n",
    "print(\"Amount of instances where the commit was reviewed/revised in inference dataset:\", inferenceDataset.revd.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spread of bugs found in the training dataset (does not include instances of NaN)\")\n",
    "sns.countplot(x='bugcount', data=trainingDataset)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugsFoundInstances = trainingDataset.bugcount.value_counts().sum() - trainingDataset.bugcount.value_counts().get(0.0)\n",
    "\n",
    "print(\"Amount of instances of bugcount containing NaN:\", trainingDataset.bugcount.isna().sum())\n",
    "print(f\"Amount of times bugs were detected: {bugsFoundInstances} out of {trainingDataset.shape[0]} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = trainingDataset.corr()\n",
    "high_correlations = correlation_matrix[(correlation_matrix > 0.9) | (correlation_matrix < -0.9)]\n",
    "\n",
    "# Print correlation for each combination of features\n",
    "for col in high_correlations.columns:\n",
    "    # filter out correlation values for columns with themselves by setting them to NaN\n",
    "    high_correlations.loc[col, col] = np.nan\n",
    "    # Drop all correlations that are set as NaN\n",
    "    correlations = high_correlations[col].dropna()\n",
    "\n",
    "    if not correlations.empty:\n",
    "        print(f\"Correlations with {col}:\")\n",
    "        print(correlations)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataset.corr()['revd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of False-values in revd:\", (trainingDataset.revd == False).sum())\n",
    "print(trainingDataset[trainingDataset['revd'] == 0].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on replacing NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances of NaN in bugcount/fixcount are assumed to imply that 0 bugs/fixes were recorded.\n",
    "\n",
    "Based on correlation analysis of ['revd'], the remaining instances of NaN are all from features related to reviews/revisions of the commit\n",
    " \n",
    "These instances are NaN iff ['revd'] = False, implying no review was made to the commit -> all review related features are therefore NaN\n",
    "\n",
    "Replacing NaN with 0 is not equally suitable for all of these features, but is done for simplicity, example:\n",
    "\n",
    "* ['app'] = 0, implies that no reviewers voted on integration when there was no review to vote on, this seems suitable\n",
    "\n",
    "* ['aexp'] = 0, implies the author has participated in 0 previous changes, which is not necessarily true, not so suitable\n",
    "\n",
    "Given more time, this could be better adapted and customized to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all \"r\" and \"o\" feature-types\n",
    "# they contain highly correlated data, increases dimensional complexity, and rank very low on feature importance when training\n",
    "unnecessaryColumns = ['arexp', 'self', 'revd', 'rexp', 'rrexp', 'rsexp', 'rsawr', 'oexp', 'orexp', 'osexp', 'osawr', 'asawr', 'asexp', 'aexp']\n",
    "trainingDataset = trainingDataset.drop(unnecessaryColumns, axis=1)\n",
    "\n",
    "# For the remaining features containing NaN, NaN are assumed to imply 0, see argument above\n",
    "trainingDataset = trainingDataset.fillna(0.0)              \n",
    "\n",
    "# turn bugcount into binary feature instead of multi-class \n",
    "trainingDataset['bugcount'] = trainingDataset['bugcount'].apply(lambda x: 1 if x != 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated cleaning for inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceDataset = inferenceDataset.drop(unnecessaryColumns, axis=1)\n",
    "inferenceDataset = inferenceDataset.fillna(0.0) \n",
    "\n",
    "inferenceDataset['bugcount'] = inferenceDataset['bugcount'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "# extract target feature ['bugcount'] for inference testing\n",
    "inference_bugcount = inferenceDataset.bugcount     \n",
    "inference_set = inferenceDataset.drop(['bugcount'], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and extract target feature before balancing \n",
    "training_data = trainingDataset\n",
    "target_feature = training_data.bugcount\n",
    "training_data = training_data.drop(['bugcount'], axis=1)\n",
    "\n",
    "# show class ratio\n",
    "print(target_feature.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing features and splitting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into test and train sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(training_data, target_feature, test_size=0.1, random_state=100)\n",
    "features_names = x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing features to values [-1, 1], can help some models accurately depict features during training\n",
    "# Example: SVM models are sensitive to the scale of features, especially SVMs with linear kernels. \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# calculates variance and mean for each feature before transforming\n",
    "x_train = sc.fit_transform(x_train) \n",
    "# transforms features using the same variance and mean on both test and inference sets\n",
    "x_test = sc.transform(x_test)  \n",
    "inference_set = sc.transform(inference_set)\n",
    "\n",
    "# transform numpyarray back into dataframe with correct feature names\n",
    "x_train = pd.DataFrame(x_train, columns=features_names)\n",
    "x_test = pd.DataFrame(x_test, columns=features_names)\n",
    "inference_set = pd.DataFrame(inference_set, columns=features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Minority Oversampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bugcount is vastly imbalanced, this is solved by increasing the ratio of instances with found bugs. \n",
    "\n",
    "* Oversampling: SMOTE chooses random samples of minority class, finds similar instances of data using k-nearest neighbor and generates a new instance between the original sample and one random neighbor.\n",
    "\n",
    "* Undersampling: Randomly undersamples data with majority class, in this case instances were bugcount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare pipeline for balancing\n",
    "# multiple different ratios for sampling strategies can be tested\n",
    "oversample = SMOTE(sampling_strategy=0.5)                        \n",
    "undersample = RandomUnderSampler(sampling_strategy=1)\n",
    "steps = [('over', oversample), ('under', undersample)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# balance bugcount class\n",
    "x_train, y_train = pipeline.fit_resample(x_train, y_train)\n",
    "# show new balanced class ratio\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier model, validated using k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify classifier model\n",
    "rfc_model = RandomForestClassifier(n_estimators=25)     #25->100, marginally better but large impact on compute\n",
    "\n",
    "# define k-fold strategy and perform crossvalidation\n",
    "crossValidation = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)     # 10 splits is a good bias-variance balance\n",
    "scores = cross_val_score(rfc_model, x_train, y_train, scoring='f1_macro', cv=crossValidation, n_jobs=-1)   \n",
    "print(f\"CrossVal mean f1_macro: {np.mean(scores): .3f}\")         # f1_macro scores adds weights to results from each class\n",
    "\n",
    "# train RandomForestClassifier model\n",
    "rfc_model.fit(x_train, y_train)\n",
    "\n",
    "# inference on test set\n",
    "rfc_predictions = rfc_model.predict(x_test)\n",
    "print(f\"Trained model f1-score: {f1_score(y_test, rfc_predictions): .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel alternatives: 'linear', 'rbf', 'poly' and 'sigmoid'\n",
    "svm_model = SVC(kernel='rbf')    \n",
    "svm_model.fit(x_train, y_train)\n",
    "svm_predictions = svm_model.predict(x_test)\n",
    "svm_f1 = f1_score(y_test, svm_predictions)\n",
    "print(\"SVM f1 score:\", svm_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes model, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This type of model also performs much better using standardized features\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(x_train, y_train)\n",
    "nb_predictions = nb_model.predict(x_test)\n",
    "nb_f1 = f1_score(y_test, nb_predictions)\n",
    "print(\"Naive Bayes f1 score:\", nb_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for RandomForestClassifier\n",
    "print(classification_report(y_test, rfc_predictions, target_names=['no bugs', 'contains bugs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for SupportVectorMachine\n",
    "print(classification_report(y_test, svm_predictions, target_names=['no bugs', 'contains bugs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for GaussianNB\n",
    "print(classification_report(y_test, nb_predictions, target_names=['no bugs', 'contains bugs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance on inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model alternatives: rfc_model, svm_model or nb_model\n",
    "rfc_inference = rfc_model.predict(inference_set)\n",
    "print(classification_report(inference_bugcount, rfc_inference, target_names=['no bugs', 'contains bugs'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_inference = svm_model.predict(inference_set)\n",
    "print(classification_report(inference_bugcount, svm_inference, target_names=['no bugs', 'contains bugs'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_inference = nb_model.predict(inference_set)\n",
    "print(classification_report(inference_bugcount, nb_inference, target_names=['no bugs', 'contains bugs'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate baseline: what are the odds of finding a bug in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline prediction for different datasets: for training_data use: target_feature | for inference_data use: inference_bugcount \n",
    "count_zero = inference_bugcount.value_counts().get(0)\n",
    "count_one = inference_bugcount.value_counts().get(1)\n",
    "totalCount = count_one + count_zero\n",
    "chance_zero = count_zero / totalCount\n",
    "chance_one = count_one / totalCount\n",
    "randomGuessAccuracy = (chance_zero + chance_one) / 2\n",
    "weightedGuessAccuracy = (chance_one * chance_one) + (chance_zero * chance_zero)\n",
    "print(f\"Bugcount values, 0: {count_zero}, 1: {count_one}\")\n",
    "print(f\"Probability of majority class, p(0): {chance_zero:.3f}\")\n",
    "print(f\"Propability of minority class, p(1): {chance_one:.3f}\")\n",
    "print(f\"Accuracy of a random guess: {randomGuessAccuracy:.3f}\")\n",
    "print(f\"Accuracy of a weighted guess: {weightedGuessAccuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(rfc_model, x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(rfc_model.feature_importances_, index=x_train.columns).sort_values(axis = 0, ascending = False)\n",
    "feature_importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
